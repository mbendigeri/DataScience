{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Mallikarjun\\\\UPXAcademylearning\\\\Jupyter\\\\DataScience\\\\TensorFlow-Deep_Learning'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Read data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use MNIST image dataset provided by tensflow library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step -1 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data as ipdata\n",
    "mnist=ipdata.read_data_sets(\"MNIST_data\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the examples above, the dataset file has 2 datasets, Train and Testt\n",
    "# in reality this will not be case. We need split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000000438D626EF0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000000438D626DD8>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000000438D626E48>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Train dataset two arrays\n",
    "## mnist.train.images is an 2D array of 55000 Images, each of the size 784 pixes (28*28) \n",
    "## mnist.train.lables is an column vector of 55000 lables, each one in representing anyone of 0- 9 digits\n",
    "## In reality we may not have a dataset in this nature. We need to create the vector of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=mnist.train.images\n",
    "trainY=mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning input features (55000, 784)\n",
      "Traning Target features (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Traning input features',trainX.shape)\n",
    "print('Traning Target features',trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX=mnist.test.images\n",
    "testY=mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning input features (10000, 784)\n",
      "Traning Target features (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Traning input features',testX.shape)\n",
    "print('Traning Target features',testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 assign the intial values for the weights and bais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a intial the parameter to save the log and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory to save the log files, graph\n",
    "logs_path='/temp/mnist/lr'\n",
    "\n",
    "## Learning rate\n",
    "learning_rate=0.03\n",
    "\n",
    "## no of features for which the model to be trained which is 784\n",
    "n_features=trainX.shape[1]\n",
    "\n",
    "\n",
    "## no of outcomes or results 10\n",
    "n_classes=trainY.shape[1]\n",
    "\n",
    "## name of the model for stroage\n",
    "model_name='mnist.ckpt'\n",
    "\n",
    "#iteration or number of times the data will be passed to model for traning\n",
    "training_epochs=60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Build the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a Define the placeholder for input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    # input data, we are taking all the rows. And has 784 features\n",
    "    x=tf.placeholder(tf.float32,shape=[None,n_features],name='x-input')\n",
    "    # outcome, it will all the rows and , 10 classes\n",
    "    y_=tf.placeholder(tf.float32,shape=[None,n_classes],name='y-input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b Define the values for wieghts and bais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y1=X.w1 + b1 # w1 here is 784 *1 matrix becos it is the weight required for 1st outcome and applied to all the 784 features\n",
    "# y2=X.w2 + b2 # w1 here is 784 *1 matrix becos it is the weight required for 2nd outcome and applied to all the 784 features\n",
    "# y3=X.w3 + b3 # w1 here is 784 *1 matrix becos it is the weight required for 3rd outcome and applied to all the 784 features\n",
    "# In genral w is a matrx of 784 (features) * 10(outcomes)\n",
    "# b is a vector of 10 outcomes as there is on bais for each out come\n",
    "# In general y is matrix of #this is 55k *10 . Matrix mutiplication of [55K by 784] X [784 by 10] results in a matrix of 55K by 10 matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape (784, 10)\n",
      "b shape (10,)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"wieghts\"):\n",
    "    # There are 784 by 10 coumns as we need wieght for each feature\n",
    "    W=tf.Variable(tf.zeros([n_features,n_classes])) #this is 784 *10 \n",
    "    b=tf.Variable(tf.zeros([n_classes])) #this is vector of 10 outcomes\n",
    "    print (\"W shape\",W.shape)\n",
    "    print (\"b shape\",b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c Predication interms of Probablity - Call the Softmaxx function to get the probabilistic result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Softmax function\n",
    "#Y=softmax(X.w + b)\n",
    "#X is the input data, a matrix of 10000*784\n",
    "#w is wieghts, a vector 10000*9 (0 to 9 numberz)\n",
    "#b is the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need normalization. Nopee as number are between 0-255. So the scale is same for all the ddta points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'output/Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"output\"):\n",
    "    y=tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d Calculate the loss(error) and reduce it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    #cross entropy loss\n",
    "    cross_entropy=tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y),reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e Gradient optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'train/GradientDescent' type=NoOp>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    train_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction Tensor(\"Accuracy/Predict:0\", shape=(?,), dtype=int64)\n",
      "correct_prediction Tensor(\"Accuracy/Equal:0\", shape=(?,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"Accuracy\"):\n",
    "    prediction=tf.argmax(y,1,name=\"Predict\")\n",
    "    print (\"prediction\",prediction)\n",
    "    correct_prediction=tf.equal(prediction,tf.argmax(y_,1))\n",
    "    print (\"correct_prediction\",correct_prediction)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuarcy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train loss and accuracy\n",
    "training_loss=tf.summary.scalar(\"training_loss\",cross_entropy)\n",
    "training_accuracy=tf.summary.scalar(\"training_accuracy\",accuracy)\n",
    "\n",
    "#Test loss and accurach\n",
    "test_loss=tf.summary.scalar(\"training_loss\",cross_entropy)\n",
    "test_accuracy=tf.summary.scalar(\"training_accuracy\",accuracy)\n",
    "\n",
    "writer=tf.summary.FileWriter(logs_path,graph=tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExecuteGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Test Accuracy 0.6815\n",
      "epoch 2\n",
      "Test Accuracy 0.6908\n",
      "epoch 3\n",
      "Test Accuracy 0.698\n",
      "epoch 4\n",
      "Test Accuracy 0.7051\n",
      "epoch 6\n",
      "Test Accuracy 0.7196\n",
      "epoch 7\n",
      "Test Accuracy 0.7253\n",
      "epoch 8\n",
      "Test Accuracy 0.7289\n",
      "epoch 9\n",
      "Test Accuracy 0.7348\n",
      "epoch 11\n",
      "Test Accuracy 0.7424\n",
      "epoch 12\n",
      "Test Accuracy 0.7459\n",
      "epoch 13\n",
      "Test Accuracy 0.7488\n",
      "epoch 14\n",
      "Test Accuracy 0.7516\n",
      "epoch 16\n",
      "Test Accuracy 0.7557\n",
      "epoch 17\n",
      "Test Accuracy 0.7578\n",
      "epoch 18\n",
      "Test Accuracy 0.7597\n",
      "epoch 19\n",
      "Test Accuracy 0.7612\n",
      "epoch 21\n",
      "Test Accuracy 0.767\n",
      "epoch 22\n",
      "Test Accuracy 0.77\n",
      "epoch 23\n",
      "Test Accuracy 0.7737\n",
      "epoch 24\n",
      "Test Accuracy 0.7752\n",
      "epoch 26\n",
      "Test Accuracy 0.7801\n",
      "epoch 27\n",
      "Test Accuracy 0.7826\n",
      "epoch 28\n",
      "Test Accuracy 0.7838\n",
      "epoch 29\n",
      "Test Accuracy 0.7853\n",
      "epoch 31\n",
      "Test Accuracy 0.7885\n",
      "epoch 32\n",
      "Test Accuracy 0.7903\n",
      "epoch 33\n",
      "Test Accuracy 0.7923\n",
      "epoch 34\n",
      "Test Accuracy 0.7935\n",
      "epoch 36\n",
      "Test Accuracy 0.7953\n",
      "epoch 37\n",
      "Test Accuracy 0.7969\n",
      "epoch 38\n",
      "Test Accuracy 0.7977\n",
      "epoch 39\n",
      "Test Accuracy 0.7985\n",
      "epoch 41\n",
      "Test Accuracy 0.8009\n",
      "epoch 42\n",
      "Test Accuracy 0.8018\n",
      "epoch 43\n",
      "Test Accuracy 0.8026\n",
      "epoch 44\n",
      "Test Accuracy 0.8038\n",
      "epoch 46\n",
      "Test Accuracy 0.8055\n",
      "epoch 47\n",
      "Test Accuracy 0.8061\n",
      "epoch 48\n",
      "Test Accuracy 0.8066\n",
      "epoch 49\n",
      "Test Accuracy 0.8083\n",
      "epoch 51\n",
      "Test Accuracy 0.8097\n",
      "epoch 52\n",
      "Test Accuracy 0.811\n",
      "epoch 53\n",
      "Test Accuracy 0.8122\n",
      "epoch 54\n",
      "Test Accuracy 0.8125\n",
      "epoch 56\n",
      "Test Accuracy 0.8137\n",
      "epoch 57\n",
      "Test Accuracy 0.814\n",
      "epoch 58\n",
      "Test Accuracy 0.8148\n",
      "epoch 59\n",
      "Test Accuracy 0.8155\n"
     ]
    }
   ],
   "source": [
    "#start the session\n",
    "with tf.Session() as sess:\n",
    "    #intializing the variables \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # perform the training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        _,acc,loss=sess.run([train_op,training_accuracy,training_loss],feed_dict={x:trainX,y_:trainY})\n",
    "\n",
    "        #log the summary\n",
    "        writer.add_summary(acc,epoch)\n",
    "        writer.add_summary(loss,epoch)\n",
    "        \n",
    "        acc,loss=sess.run([test_accuracy,test_loss],feed_dict={x:testX,y_:testY})\n",
    "        \n",
    "        writer.add_summary(acc,epoch)\n",
    "        writer.add_summary(loss,epoch)\n",
    "        #for every 10 rounds of training\n",
    "        if (epoch%5):\n",
    "            print(\"epoch\",epoch)\n",
    "            print(\"Test Accuracy\",accuracy.eval(feed_dict={x:testX,y_:testY}))\n",
    "            \n",
    "    #Create a Saver to save the graph\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, logs_path + '/' + model_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
